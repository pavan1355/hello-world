# -*- coding: utf-8 -*-
"""data_science_project_customer_segmentation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18WuZvQRSWa9XiY_OWCc5uRemWLO5G3St
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import datetime
import warnings

# For text processing
import nltk
nltk.download('punkt') # Download necessary resource for tokenization
nltk.download('averaged_perceptron_tagger') #Download necessary for pos_tag

# For clustering and evaluation
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# For dimensionality reduction
from sklearn.decomposition import PCA

# For word clouds
from wordcloud import WordCloud, STOPWORDS

# For interactive plots
import plotly.express as px # Using plotly.express for easier interactive plots

# Suppress warnings
warnings.filterwarnings("ignore")

# Set style for matplotlib plots
plt.style.use('fivethirtyeight')

# Configure Plotly to work offline in Jupyter Notebook
import plotly.io as pio
pio.renderers.default = "notebook_connected" # Use "notebook_connected" for interactive mode

data = pd.read_csv('data.csv', encoding='ISO-8859-1')

data.shape

data.head()

# I will have to drop the missing values/rows from the CustomerID
data.dropna(axis = 0, subset = ['CustomerID'], inplace = True)
print('Dataframe dimensions:', data.shape)

print('Duplicate Entries: {}'.format(data.duplicated().sum()))
data.drop_duplicates(inplace = True)
data_cleaned = data.copy()

data_cleaned['InvoiceDate'] = pd.to_datetime(data_cleaned['InvoiceDate'])
data_cleaned['Year'] = data_cleaned['InvoiceDate'].dt.year
data_cleaned['Month'] = data_cleaned['InvoiceDate'].dt.month
data_cleaned['Day'] = data_cleaned['InvoiceDate'].dt.day
data_cleaned['Hour'] = data_cleaned['InvoiceDate'].dt.hour

data_cleaned = data_cleaned[(data_cleaned['Quantity'] > 0) & (data_cleaned['UnitPrice'] > 0)]

# Calculate TotalPrice
data_cleaned['TotalPrice'] = data_cleaned['Quantity'] * data_cleaned['UnitPrice']

data_cleaned.head()

temp_cou = data_cleaned[['CustomerID', 'InvoiceNo', 'Country']].groupby(['CustomerID', 'InvoiceNo', 'Country']).count()

temp_cou

# Add an index number
# reset_index() is a method to reset index of a Data Frame.
# reset_index() method sets a list of integer ranging from 0 to length of data as index.
temp_cou = temp_cou.reset_index(drop = False)

# Take count of the number of unique countries
countries = temp_cou['Country'].value_counts()
print('No. of countries in the dataframe: {}'.format(len(countries)))

countries.index

countries

len(data_cleaned['CustomerID'].value_counts())

pd.DataFrame([{'products': len(data_cleaned['StockCode'].value_counts()),
               'transactions': len(data_cleaned['InvoiceNo'].value_counts()),
               'customers': len(data_cleaned['CustomerID'].value_counts()),}],
             columns = ['products', 'transactions', 'customers'], index = ['quantity'])

temp_pro = data_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()

# Rename the InvoiceDate to number of products
products_per_cart = temp_pro.rename(columns = {'InvoiceDate':'Number of products'})

# Sort in Ascending order based on CustomerID
products_per_cart[:10].sort_values('CustomerID') # List first 10 values

# sum of purchases / user & order
temp_sum = data_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()
cart_price = temp_sum.rename(columns = {'TotalPrice':'cart Price'})

# date of the order
data_cleaned['InvoiceDate_int'] = data_cleaned['InvoiceDate'].astype('int64')
temp_date = data_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()
data_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)
cart_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp_date['InvoiceDate_int'])

# selection of significant entries:
cart_price = cart_price[cart_price['cart Price'] > 0] # Changed basket_price to cart_price
cart_price.sort_values('CustomerID')[:6] # Changed np.character_price to cart_price

cart_price.max(), cart_price.min()

# Purchase count
price_range = [0, 50, 100, 200, 500, 1000, 5000, 50000]

count_price = []

for i, price in enumerate(price_range):
    if i == 0: continue
    # Use cart_price instead of basket_price for filtering
    val = cart_price[(cart_price['cart Price'] < price) &
                       (cart_price['cart Price'] > price_range[i-1])]['cart Price'].count()
    count_price.append(val)



# Representation of the number of purchases / amount
plt.rc('font', weight='bold')
f, ax = plt.subplots(figsize=(11, 6))
colors = ['yellowgreen', 'gold', 'wheat', 'c', 'violet', 'royalblue','firebrick']
labels = [ '{}<.<{}'.format(price_range[i-1], s) for i,s in enumerate(price_range) if i != 0]
sizes  = count_price
explode = [0.0 if sizes[i] < 100 else 0.0 for i in range(len(sizes))]
ax.pie(sizes, explode = explode, labels=labels, colors = colors,
       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',
       shadow = False, startangle=0)
ax.axis('equal')
f.text(0.5, 1.01, "Distribution of order amounts", ha='center', fontsize = 18);

# Monthly sales
monthly_sales = data_cleaned.groupby('Month')['TotalPrice'].sum()
plt.figure(figsize=(12, 6))
monthly_sales.plot(kind='line', marker='o', color='green')
plt.title("Monthly Sales Trend")
plt.xlabel("Month")
plt.ylabel("Total Sales")
plt.show()

# Day of week analysis
data_cleaned['DayOfWeek'] = data_cleaned['InvoiceDate'].dt.day_name()
plt.figure(figsize=(10, 6))
sns.countplot(data=data_cleaned, x='DayOfWeek', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])
plt.title("Transaction Count by Day of the Week")
plt.show()

# Identify high-selling products
product_sales = data_cleaned.groupby('Description')['Quantity'].sum().sort_values(ascending=False)
top_selling_products = product_sales.head(10)  # Get the top 10
print("Top 10 selling products:\n", top_selling_products)

# Plot top 10 selling products
plt.figure(figsize=(12, 6))
top_selling_products.plot(kind='bar', color='skyblue')
plt.title("Top 10 Selling Products by Quantity")
plt.xlabel("Product Description")
plt.ylabel("Total Quantity Sold")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Calculate and plot top 10 products by revenue
product_revenue = data_cleaned.groupby('Description')['TotalPrice'].sum().sort_values(ascending=False)
top_revenue_products = product_revenue.head(10)
print("\nTop 10 products by revenue:\n", top_revenue_products)

# Plot top 10 products by revenue
plt.figure(figsize=(12, 6))
top_revenue_products.plot(kind='bar', color='lightgreen')
plt.title("Top 10 Products by Revenue")
plt.xlabel("Product Description")
plt.ylabel("Total Revenue")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# prompt: give me kde plot for of customers and invoice purchases

# Calculate the number of purchases per customer
customer_purchases = data_cleaned.groupby('CustomerID')['InvoiceNo'].nunique()

# Create the KDE plot
plt.figure(figsize=(10, 6))
sns.kdeplot(customer_purchases, shade=True)
plt.title('Distribution of Customer Purchases')
plt.xlabel('Number of Purchases')
plt.ylabel('Density')
plt.show()



df_recency = data_cleaned.groupby(by='CustomerID',
						as_index=False)['InvoiceDate'].max()
df_recency.columns = ['CustomerID', 'LastPurchaseDate']
recent_date = df_recency['LastPurchaseDate'].max()
df_recency['Recency'] = df_recency['LastPurchaseDate'].apply(
	lambda x: (recent_date - x).days)
df_recency.head()

frequency_df = data_cleaned.drop_duplicates().groupby(
	by=['CustomerID'], as_index=False)['InvoiceDate'].count()
frequency_df.columns = ['CustomerID', 'Frequency']
frequency_df.head()

# Calculate TotalPrice
data_cleaned['TotalPrice'] = data_cleaned['Quantity'] * data_cleaned['UnitPrice']
monetary_df = data_cleaned.groupby(by='CustomerID', as_index=False)['TotalPrice'].sum()
monetary_df.columns = ['CustomerID', 'Monetary']
monetary_df.head()

# @title CustomerID vs Monetary

from matplotlib import pyplot as plt
monetary_df.plot(kind='scatter', x='CustomerID', y='Monetary', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

import pandas as pd

# ... (Your previous code for loading and preprocessing data) ...

# Calculate Recency
df_recency = data_cleaned.groupby(by='CustomerID', as_index=False)['InvoiceDate'].max()
df_recency.columns = ['CustomerID', 'LastPurchaseDate']
recent_date = df_recency['LastPurchaseDate'].max()
df_recency['Recency'] = df_recency['LastPurchaseDate'].apply(lambda x: (recent_date - x).days)

# Calculate Frequency
frequency_df = data_cleaned.drop_duplicates().groupby(by=['CustomerID'], as_index=False)['InvoiceDate'].count()
frequency_df.columns = ['CustomerID', 'Frequency']

# Calculate Monetary
data_cleaned['TotalPrice'] = data_cleaned['Quantity'] * data_cleaned['UnitPrice']  # Calculate TotalPrice if not already done
monetary_df = data_cleaned.groupby(by='CustomerID', as_index=False)['TotalPrice'].sum()
monetary_df.columns = ['CustomerID', 'Monetary']

# Now you can merge the DataFrames
rf_df = df_recency.merge(frequency_df, on='CustomerID')
rfm_df = rf_df.merge(monetary_df, on='CustomerID').drop(columns='LastPurchaseDate')

rfm_df.head()

# @title CustomerID vs Recency

from matplotlib import pyplot as plt
rfm_df.plot(kind='scatter', x='CustomerID', y='Recency', s=32, alpha=.8)
plt.gca().spines[['top', 'right',]].set_visible(False)

rfm_df['R_rank'] = rfm_df['Recency'].rank(ascending=False)
rfm_df['F_rank'] = rfm_df['Frequency'].rank(ascending=True)
rfm_df['M_rank'] = rfm_df['Monetary'].rank(ascending=True)

# normalizing the rank of the customers
rfm_df['R_rank_norm'] = (rfm_df['R_rank']/rfm_df['R_rank'].max())*100
rfm_df['F_rank_norm'] = (rfm_df['F_rank']/rfm_df['F_rank'].max())*100
rfm_df['M_rank_norm'] = (rfm_df['F_rank']/rfm_df['M_rank'].max())*100

rfm_df.drop(columns=['R_rank', 'F_rank', 'M_rank'], inplace=True)

rfm_df.head()

rfm_df['RFM_Score'] = 0.15*rfm_df['R_rank_norm']+0.28 * \
	rfm_df['F_rank_norm']+0.57*rfm_df['M_rank_norm']
rfm_df['RFM_Score'] *= 0.05
rfm_df = rfm_df.round(2)
rfm_df[['CustomerID', 'RFM_Score']].head(7)

rfm_df["Customer_segment"] = np.where(rfm_df['RFM_Score'] >
									4.5, "Top Customers",
									(np.where(
										rfm_df['RFM_Score'] > 4,
										"High value Customer",
										(np.where(
	rfm_df['RFM_Score'] > 3,
							"Medium Value Customer",
							np.where(rfm_df['RFM_Score'] > 1.6,
							'Low Value Customers', 'Lost Customers'))))))
rfm_df[['CustomerID', 'RFM_Score', 'Customer_segment']].head(20)

from matplotlib import pyplot as plt
import seaborn as sns

# Replace '_df_2' with 'rfm_df'
rfm_df.groupby('Customer_segment').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

plt.pie(rfm_df.Customer_segment.value_counts(),labels=rfm_df.Customer_segment.value_counts().index,autopct='%.0f%%')
plt.show()

rfm_df.head()

from sklearn.preprocessing import StandardScaler

# Standardize Recency, Frequency, and Monetary
rfm_df[['Recency', 'Frequency', 'Monetary']] = StandardScaler().fit_transform(rfm_df[['Recency', 'Frequency', 'Monetary']])

rfm_df.head()

# prompt: Use K-Means clustering to segment customers.
from sklearn.cluster import KMeans

# Apply KMeans clustering and assign cluster labels
rfm_df['Cluster'] = KMeans(n_clusters=5, random_state=0).fit_predict(rfm_df[['Recency', 'Frequency', 'Monetary']])

# Analyze clusters
print(rfm_df.groupby('Cluster')[['Recency', 'Frequency', 'Monetary']].mean())

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np

# Assuming rfm_df is already created as in your provided code
# ... (Your previous code for loading and preprocessing data and creating rfm_df) ...


# Standardize Recency, Frequency, and Monetary (if not already done)
rfm_df[['Recency', 'Frequency', 'Monetary']] = StandardScaler().fit_transform(rfm_df[['Recency', 'Frequency', 'Monetary']])

# Apply KMeans clustering with the optimal number of clusters (e.g., 5)
kmeans = KMeans(n_clusters=5, random_state=42)
rfm_df['Cluster'] = kmeans.fit_predict(rfm_df[['Recency', 'Frequency', 'Monetary']])

# Plot the clusters using PCA for visualization
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
pca_result = pca.fit_transform(rfm_df[['Recency', 'Frequency', 'Monetary']])

plt.figure(figsize=(8, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=rfm_df['Cluster'], cmap='viridis')
plt.title('KMeans Clustering Visualization (PCA)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster')
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Determine the optimal number of clusters using the Elbow Method
k_range = range(2, 11)
inertia = []

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42).fit(rfm_df[['Recency', 'Frequency', 'Monetary']])
    inertia.append(kmeans.inertia_)

# Plot the Elbow Curve
plt.figure(figsize=(8, 5))
plt.plot(k_range, inertia, marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of clusters (k)')
plt.ylabel('Inertia')
plt.show()

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Standardize Recency, Frequency, and Monetary
rfm_df[['Recency', 'Frequency', 'Monetary']] = StandardScaler().fit_transform(rfm_df[['Recency', 'Frequency', 'Monetary']])

# Apply KMeans clustering with the optimal k
rfm_df['Cluster'] = KMeans(n_clusters=5, random_state=42).fit_predict(rfm_df[['Recency', 'Frequency', 'Monetary']])

# Display the updated DataFrame with cluster assignments
print(rfm_df.head())

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Define features and target, split data
X = rfm_df[['Recency', 'Frequency', 'Monetary']]
y = rfm_df['Cluster']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train and evaluate the model
model = LogisticRegression().fit(X_train, y_train)
accuracy = accuracy_score(y_test, model.predict(X_test))
print(f"Accuracy: {accuracy}")

# prompt: Train a Random Forest Classifier

from sklearn.ensemble import RandomForestClassifier

# Initialize and train the Random Forest Classifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust n_estimators
rf_model.fit(X_train, y_train)

# Make predictions on the test set
rf_y_pred = rf_model.predict(X_test)

# Evaluate the model
rf_accuracy = accuracy_score(y_test, rf_y_pred)
print(f"Random Forest Accuracy: {rf_accuracy}")

# prompt: perform a dbscan clusttering

from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler

# Assuming rfm_df is already created and standardized as in your provided code
# ... (Your previous code for loading, preprocessing, and creating rfm_df) ...

# Standardize the features if not already done
rfm_df[['Recency', 'Frequency', 'Monetary']] = StandardScaler().fit_transform(rfm_df[['Recency', 'Frequency', 'Monetary']])

# Apply DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5) # You can adjust eps and min_samples
rfm_df['DBSCAN_Cluster'] = dbscan.fit_predict(rfm_df[['Recency', 'Frequency', 'Monetary']])

# Analyze the clusters
print(rfm_df.groupby('DBSCAN_Cluster')[['Recency', 'Frequency', 'Monetary']].mean())

# Visualize the clusters (using PCA for dimensionality reduction)
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA(n_components=2)
pca_result = pca.fit_transform(rfm_df[['Recency', 'Frequency', 'Monetary']])

plt.figure(figsize=(8, 6))
plt.scatter(pca_result[:, 0], pca_result[:, 1], c=rfm_df['DBSCAN_Cluster'], cmap='viridis')
plt.title('DBSCAN Clustering Visualization (PCA)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Cluster')
plt.show()

# prompt: Identify the most important features influencing customer behavior.

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# Assuming rfm_df is already created and standardized as in your provided code
# ... (Your previous code for loading, preprocessing, and creating rfm_df) ...

# Feature Importance from KMeans Clustering

# Standardize the features if not already done
rfm_df[['Recency', 'Frequency', 'Monetary']] = StandardScaler().fit_transform(rfm_df[['Recency', 'Frequency', 'Monetary']])


# Apply KMeans clustering with a suitable number of clusters
kmeans = KMeans(n_clusters=5, random_state=42) # Example: 5 clusters
rfm_df['Cluster'] = kmeans.fit_predict(rfm_df[['Recency', 'Frequency', 'Monetary']])

# Get feature importances from the fitted KMeans model (inertia reduction)
feature_importances = abs(kmeans.cluster_centers_) #Absolute values of cluster center coordinates

# Create a DataFrame to visualize feature importances
importance_df = pd.DataFrame(feature_importances, columns=['Recency', 'Frequency', 'Monetary'])
importance_df['Cluster'] = range(len(importance_df))
importance_df = importance_df.melt(id_vars='Cluster', var_name='Feature', value_name='Importance')

# prompt: Use predictive modeling to identify customers likely to churn based on RFM metrics and behavioral patterns.

# Assuming rfm_df is already created and standardized as in your provided code
# ... (Your previous code for loading, preprocessing, and creating rfm_df) ...

# Prepare the data for churn prediction
# Assuming 'Customer_segment' is a categorical feature indicating churn risk
# Convert 'Customer_segment' into numerical labels if necessary
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
rfm_df['Customer_segment'] = le.fit_transform(rfm_df['Customer_segment'])

# Define features (X) and target variable (y)
X = rfm_df[['Recency', 'Frequency', 'Monetary']]  # Use RFM features as predictors
y = rfm_df['Customer_segment']  # Target variable: customer segment (churn risk)


# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Train a predictive model (e.g., Logistic Regression)
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy}")

# You can further improve the model by trying different algorithms (RandomForest, SVM, etc.)
# and tuning hyperparameters.  Also consider feature engineering.

# prompt: give me code to deploy the model in streamlit for predictive classifier

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import joblib # for saving and loading the model


# Load the trained model (replace 'trained_model.joblib' with your actual model file)
try:
    model = joblib.load('trained_model.joblib')
    scaler = joblib.load('scaler.joblib')
except FileNotFoundError:
    st.error("Model file not found. Please upload the 'trained_model.joblib' and 'scaler.joblib' files.")
    st.stop()


st.title("Customer Segmentation Prediction")

# Input features
recency = st.number_input("Recency", value=0.0)
frequency = st.number_input("Frequency", value=0.0)
monetary = st.number_input("Monetary", value=0.0)


# Create input DataFrame
input_data = pd.DataFrame({'Recency': [recency], 'Frequency': [frequency], 'Monetary': [monetary]})

# Scale the input using the same scaler used for training
input_data_scaled = scaler.transform(input_data)

# Make prediction
prediction = model.predict(input_data_scaled)[0]


# Display the prediction
st.write(f"Predicted Customer Segment: {prediction}")



# Example code to save the model (run this once after training)
# import joblib
# joblib.dump(model, 'trained_model.joblib')
# joblib.dump(scaler, 'scaler.joblib')

pip install streamlit

# ... (Your other imports)

# Load the trained model and the scaler
try:
    model = joblib.load('trained_model.joblib')
    scaler = joblib.load('scaler.joblib')  # Load the scaler
except FileNotFoundError:
    st.error("Model or scaler file not found. Please train the model and save both 'trained_model.joblib' and 'scaler.joblib' files.")
    st.stop()

# ... (Rest of your Streamlit code)

# Example code to save the model and scaler (run this once after training)
# import joblib
# joblib.dump(model, 'trained_model.joblib')
# joblib.dump(scaler, 'scaler.joblib') # Save the scaler

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import joblib # for saving and loading the model

# Assume you have trained a model and a scaler before this
# and stored them in the files 'trained_model.joblib' and 'scaler.joblib'

# Initialize the StandardScaler (if it hasn't been done before)
scaler = StandardScaler()

# ... your model training code ...
# Assuming rfm_df has been scaled using the scaler

# ... (Your previous code for loading and preprocessing data and creating rfm_df) ...

# Prepare the data for churn prediction
# Assuming 'Customer_segment' is a categorical feature indicating churn risk
# Convert 'Customer_segment' into numerical labels if necessary
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
rfm_df['Customer_segment'] = le.fit_transform(rfm_df['Customer_segment'])

# Define features (X) and target variable (y)
X = rfm_df[['Recency', 'Frequency', 'Monetary']]  # Use RFM features as predictors
y = rfm_df['Customer_segment']  # Target variable: customer segment (churn risk)


# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Train a predictive model (e.g., Logistic Regression)
model = LogisticRegression()
model.fit(X_train, y_train)


# Save the trained model and scaler
joblib.dump(model, 'trained_model.joblib')
joblib.dump(scaler, 'scaler.joblib') # Save the scaler

# ... (continue with your Streamlit code) ...
# Load the trained model (replace 'trained_model.joblib' with your actual model file)
try:
    model = joblib.load('trained_model.joblib')
    scaler = joblib.load('scaler.joblib')  # Load the scaler
except FileNotFoundError:
    st.error("Model or scaler file not found. Please train the model and save both 'trained_model.joblib' and 'scaler.joblib' files.")
    st.stop()


st.title("Customer Segmentation Prediction")

# Input features
recency = st.number_input("Recency", value=0.0)
frequency = st.number_input("Frequency", value=0.0)
monetary = st.number_input("Monetary", value=0.0)


# Create input DataFrame
input_data = pd.DataFrame({'Recency': [recency], 'Frequency': [frequency], 'Monetary': [monetary]})

# Scale the input using the same scaler used for training
input_data_scaled = scaler.transform(input_data)

# Make prediction
prediction = model.predict(input_data_scaled)[0]


# Display the prediction
st.write(f"Predicted Customer Segment: {prediction}")



# Example code to save the model (run this once after training)
# import joblib
# joblib.dump(model, 'trained_model.joblib')
# joblib.dump(scaler, 'scaler.joblib') # Save the scaler

import streamlit as st
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import joblib # for saving and loading the model

# Assume you have trained a model and a scaler before this
# and stored them in the files 'trained_model.joblib' and 'scaler.joblib'

# ... your model training code ...
# Assuming rfm_df has been scaled using the scaler

# ... (Your previous code for loading and preprocessing data and creating rfm_df) ...

# Prepare the data for churn prediction
# Assuming 'Customer_segment' is a categorical feature indicating churn risk
# Convert 'Customer_segment' into numerical labels if necessary
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
rfm_df['Customer_segment'] = le.fit_transform(rfm_df['Customer_segment'])

# Define features (X) and target variable (y)
X = rfm_df[['Recency', 'Frequency', 'Monetary']]  # Use RFM features as predictors
y = rfm_df['Customer_segment']  # Target variable: customer segment (churn risk)


# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the StandardScaler
scaler = StandardScaler()

#Fit the scaler to your training data
scaler.fit(X_train) #Fit the scaler before training the model

# Transform the training and testing data using the fitted scaler
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)


# Train a predictive model (e.g., Logistic Regression)
model = LogisticRegression()
model.fit(X_train_scaled, y_train) #Train the model with scaled data


# Save the trained model and scaler
joblib.dump(model, 'trained_model.joblib')
joblib.dump(scaler, 'scaler.joblib') # Save the scaler

# ... (continue with your Streamlit code) ...
# Load the trained model (replace 'trained_model.joblib' with your actual model file)
try:
    model = joblib.load('trained_model.joblib')
    scaler = joblib.load('scaler.joblib')  # Load the scaler
except FileNotFoundError:
    st.error("Model or scaler file not found. Please train the model and save both 'trained_model.joblib' and 'scaler.joblib' files.")
    st.stop()


st.title("Customer Segmentation Prediction")

# Input features
recency = st.number_input("Recency", value=0.0)
frequency = st.number_input("Frequency", value=0.0)
monetary = st.number_input("Monetary", value=0.0)


# Create input DataFrame
input_data = pd.DataFrame({'Recency': [recency], 'Frequency': [frequency], 'Monetary': [monetary]})

# Scale the input using the same scaler used for training
input_data_scaled = scaler.transform(input_data)

# Make prediction
prediction = model.predict(input_data_scaled)[0]


# Display the prediction
st.write(f"Predicted Customer Segment: {prediction}")



# Example code to save the model (run this once after training)
# import joblib
# joblib.dump(model, 'trained_model.joblib')
# joblib.dump(scaler, 'scaler.joblib') # Save the scaler

# prompt: now deploy in streamlit

!pip install streamlit
!pip install joblib
!pip install pandas
!pip install numpy
!pip install matplotlib
!pip install seaborn
!pip install scikit-learn
!pip install plotly
!pip install wordcloud

import streamlit as st
import joblib
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler


# Load the trained model and the scaler
try:
    model = joblib.load('trained_model.joblib')
    scaler = joblib.load('scaler.joblib')
except FileNotFoundError:
    st.error("Model or scaler file not found. Please train the model and save both 'trained_model.joblib' and 'scaler.joblib' files.")
    st.stop()

st.title("Customer Segmentation Prediction")

# Input features
recency = st.number_input("Recency", value=0.0)
frequency = st.number_input("Frequency", value=0.0)
monetary = st.number_input("Monetary", value=0.0)

# Create input DataFrame
input_data = pd.DataFrame({'Recency': [recency], 'Frequency': [frequency], 'Monetary': [monetary]})

# Scale the input using the same scaler used for training
input_data_scaled = scaler.transform(input_data)

# Make prediction
prediction = model.predict(input_data_scaled)[0]

# Display the prediction
st.write(f"Predicted Customer Segment: {prediction}")

# prompt: now run it in streamlit app

import streamlit as st
import joblib
import pandas as pd
from sklearn.preprocessing import StandardScaler

def is_woodall(x):
  if (x % 2 == 0):
    return False
  if (x == 1):
    return True
  x = x + 1
  p = 0
  while (x % 2 == 0):
    x = x/2
    p = p + 1
    if (p == x):
      return True
  return False

st.title("Woodall Number Checker")

number = st.number_input("Enter a number:", min_value=0, value=1)

if st.button("Check"):
    if is_woodall(int(number)):
        st.write(f"{number} is a Woodall number.")
    else:
        st.write(f"{number} is not a Woodall number.")



